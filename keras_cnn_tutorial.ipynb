{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification - From MLPs to ConvNets\n",
    "\n",
    "For this practical we are going to train a small network for digit classification on images. \n",
    "\n",
    "First, we are going to train an MLP classifier, using only Dense layers (and dropout). \n",
    "\n",
    "Next, we are going to replace the MLP with a ConvNet, using Convolutional Layers, Max Pooling etc. \n",
    "\n",
    "Your task is to try variations of the above and compare their performance. Do pay attention to the order of layers in ConvNets and their importance in performance increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras makes it very easy to use the MNIST dataset by simply loading it. Explore the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the 58118th sample with label 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24efff7ffc8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#% matplotlib inline\n",
    "import random\n",
    "\n",
    "n_th_image = random.randint(0, x_train.shape[0])  # We print a random image every time\n",
    "print(f\"Printing the {n_th_image}th sample with label {y_train[n_th_image]}\")\n",
    "plt.imshow(x_train[n_th_image], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some technicalities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "print('x_train shape:', x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "mlp_model = Sequential()\n",
    "mlp_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dropout(0.2))\n",
    "mlp_model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sindi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.7864 - accuracy: 0.7620 - val_loss: 0.3197 - val_accuracy: 0.9079\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.3306 - accuracy: 0.9042 - val_loss: 0.2400 - val_accuracy: 0.9284\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2628 - accuracy: 0.9239 - val_loss: 0.1991 - val_accuracy: 0.9411\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2237 - accuracy: 0.9344 - val_loss: 0.1773 - val_accuracy: 0.9468\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1980 - accuracy: 0.9434 - val_loss: 0.1586 - val_accuracy: 0.9524\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1769 - accuracy: 0.9481 - val_loss: 0.1582 - val_accuracy: 0.9519\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.1611 - accuracy: 0.9533 - val_loss: 0.1389 - val_accuracy: 0.9582\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.1483 - accuracy: 0.9567 - val_loss: 0.1312 - val_accuracy: 0.9597\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.1352 - accuracy: 0.9606 - val_loss: 0.1201 - val_accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.1256 - accuracy: 0.9635 - val_loss: 0.1148 - val_accuracy: 0.9654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16e3fa59e48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.1252 - accuracy: 0.9625 - val_loss: 0.1165 - val_accuracy: 0.9648\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1179 - accuracy: 0.9654 - val_loss: 0.1074 - val_accuracy: 0.9673\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1117 - accuracy: 0.9663 - val_loss: 0.1045 - val_accuracy: 0.9687\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 0.1056 - accuracy: 0.9680 - val_loss: 0.0991 - val_accuracy: 0.9696\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0998 - accuracy: 0.9701 - val_loss: 0.1000 - val_accuracy: 0.9685\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0958 - accuracy: 0.9716 - val_loss: 0.0979 - val_accuracy: 0.9689\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.0914 - accuracy: 0.9728 - val_loss: 0.0935 - val_accuracy: 0.9713\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0866 - accuracy: 0.9744 - val_loss: 0.0943 - val_accuracy: 0.9712\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0828 - accuracy: 0.9749 - val_loss: 0.0929 - val_accuracy: 0.9697\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0795 - accuracy: 0.9764 - val_loss: 0.0917 - val_accuracy: 0.9729\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0762 - accuracy: 0.9774 - val_loss: 0.0914 - val_accuracy: 0.9710\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 0.0734 - accuracy: 0.9778 - val_loss: 0.0905 - val_accuracy: 0.9714\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0707 - accuracy: 0.9788 - val_loss: 0.0894 - val_accuracy: 0.9732\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0666 - accuracy: 0.9800 - val_loss: 0.0917 - val_accuracy: 0.9728\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0666 - accuracy: 0.9798 - val_loss: 0.0936 - val_accuracy: 0.9725\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 0.0624 - accuracy: 0.9810 - val_loss: 0.0866 - val_accuracy: 0.9733\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0594 - accuracy: 0.9821 - val_loss: 0.0876 - val_accuracy: 0.9729\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0570 - accuracy: 0.9836 - val_loss: 0.0853 - val_accuracy: 0.9740\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 0.0563 - accuracy: 0.9830 - val_loss: 0.0907 - val_accuracy: 0.9724\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0546 - accuracy: 0.9832 - val_loss: 0.0864 - val_accuracy: 0.9730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x177745b2b08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1: Build your own MLP model\n",
    "Try to use different amount of dense layers, variations of dropout etc. Report your architecture with the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.7807 - accuracy: 0.8186 - val_loss: 0.4779 - val_accuracy: 0.8805\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.4554 - accuracy: 0.8807 - val_loss: 0.3989 - val_accuracy: 0.8941\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.4025 - accuracy: 0.8917 - val_loss: 0.3660 - val_accuracy: 0.9018\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3761 - accuracy: 0.8968 - val_loss: 0.3476 - val_accuracy: 0.9069\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3594 - accuracy: 0.9008 - val_loss: 0.3348 - val_accuracy: 0.9104\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3478 - accuracy: 0.9034 - val_loss: 0.3253 - val_accuracy: 0.9126\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3387 - accuracy: 0.9053 - val_loss: 0.3198 - val_accuracy: 0.9124\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.3317 - accuracy: 0.9079 - val_loss: 0.3133 - val_accuracy: 0.9142\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.3259 - accuracy: 0.9095 - val_loss: 0.3091 - val_accuracy: 0.9155\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3210 - accuracy: 0.9107 - val_loss: 0.3055 - val_accuracy: 0.9159\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3168 - accuracy: 0.9118 - val_loss: 0.3030 - val_accuracy: 0.9168\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3132 - accuracy: 0.9133 - val_loss: 0.2995 - val_accuracy: 0.9175\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3100 - accuracy: 0.9141 - val_loss: 0.2976 - val_accuracy: 0.9186\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 41us/step - loss: 0.3072 - accuracy: 0.9145 - val_loss: 0.2948 - val_accuracy: 0.9187\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.3045 - accuracy: 0.9155 - val_loss: 0.2946 - val_accuracy: 0.9187\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.3024 - accuracy: 0.9161 - val_loss: 0.2914 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3003 - accuracy: 0.9166 - val_loss: 0.2906 - val_accuracy: 0.9189\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2983 - accuracy: 0.9170 - val_loss: 0.2885 - val_accuracy: 0.9205\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.2965 - accuracy: 0.9173 - val_loss: 0.2881 - val_accuracy: 0.9201\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2949 - accuracy: 0.9178 - val_loss: 0.2869 - val_accuracy: 0.9213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x177746b5948>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp_model = Sequential()\n",
    "my_mlp_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "###### YOUR LAYERS HERE\n",
    "\n",
    "mlp_model.add(Dense(32, activation='relu'))\n",
    "mlp_model.add(Dense(32, activation='tanh'))\n",
    "mlp_model.add(Dropout(0.20))\n",
    "mlp_model.add(Dense(16, activation='relu'))\n",
    "mlp_model.add(Dense(16, activation='tanh'))\n",
    "mlp_model.add(Dropout(0.20))\n",
    "\n",
    "# my_mlp_model.add(Dense(??, activation=??))\n",
    "# my_mlp_model.add(Dropout(??))\n",
    "\n",
    "############\n",
    "my_mlp_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "my_mlp_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "my_mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.4682 - accuracy: 0.8777 - val_loss: 0.3076 - val_accuracy: 0.9145\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.3033 - accuracy: 0.9155 - val_loss: 0.2868 - val_accuracy: 0.9194\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2834 - accuracy: 0.9205 - val_loss: 0.2746 - val_accuracy: 0.9233\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.2731 - accuracy: 0.9236 - val_loss: 0.2670 - val_accuracy: 0.9255\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2661 - accuracy: 0.9259 - val_loss: 0.2661 - val_accuracy: 0.9257\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2616 - accuracy: 0.9276 - val_loss: 0.2654 - val_accuracy: 0.9256\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2585 - accuracy: 0.9276 - val_loss: 0.2662 - val_accuracy: 0.9268\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2548 - accuracy: 0.9303 - val_loss: 0.2631 - val_accuracy: 0.9276\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2528 - accuracy: 0.9299 - val_loss: 0.2681 - val_accuracy: 0.9273\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2510 - accuracy: 0.9304 - val_loss: 0.2621 - val_accuracy: 0.9279\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.2493 - accuracy: 0.9323 - val_loss: 0.2658 - val_accuracy: 0.9263\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.2472 - accuracy: 0.9314 - val_loss: 0.2634 - val_accuracy: 0.9285\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.2462 - accuracy: 0.9319 - val_loss: 0.2684 - val_accuracy: 0.9255\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.2448 - accuracy: 0.9323 - val_loss: 0.2671 - val_accuracy: 0.9262\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2438 - accuracy: 0.9329 - val_loss: 0.2645 - val_accuracy: 0.9278\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.2425 - accuracy: 0.9328 - val_loss: 0.2653 - val_accuracy: 0.9268\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.2420 - accuracy: 0.9333 - val_loss: 0.2647 - val_accuracy: 0.9270\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2411 - accuracy: 0.9333 - val_loss: 0.2657 - val_accuracy: 0.9276\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2403 - accuracy: 0.9337 - val_loss: 0.2731 - val_accuracy: 0.9253\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2400 - accuracy: 0.9343 - val_loss: 0.2711 - val_accuracy: 0.9271\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2389 - accuracy: 0.9347 - val_loss: 0.2713 - val_accuracy: 0.9261\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2383 - accuracy: 0.9343 - val_loss: 0.2694 - val_accuracy: 0.9282\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.2381 - accuracy: 0.9342 - val_loss: 0.2693 - val_accuracy: 0.9274\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2372 - accuracy: 0.9347 - val_loss: 0.2681 - val_accuracy: 0.9287\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2369 - accuracy: 0.9352 - val_loss: 0.2726 - val_accuracy: 0.9260\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2366 - accuracy: 0.9350 - val_loss: 0.2701 - val_accuracy: 0.9279\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.2357 - accuracy: 0.9347 - val_loss: 0.2733 - val_accuracy: 0.9269\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.2351 - accuracy: 0.9354 - val_loss: 0.2742 - val_accuracy: 0.9275\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2343 - accuracy: 0.9355 - val_loss: 0.2738 - val_accuracy: 0.9254\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2340 - accuracy: 0.9357 - val_loss: 0.2746 - val_accuracy: 0.9279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2013379ce08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp_model = Sequential()\n",
    "my_mlp_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "###### YOUR LAYERS HERE\n",
    "\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dense(64, activation='tanh'))\n",
    "mlp_model.add(Dense(64, activation='sigmoid'))\n",
    "mlp_model.add(Dropout(0.20))\n",
    "\n",
    "# my_mlp_model.add(Dense(??, activation=??))\n",
    "# my_mlp_model.add(Dropout(??))\n",
    "\n",
    "############\n",
    "my_mlp_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "my_mlp_model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "my_mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.4701 - accuracy: 0.8760 - val_loss: 0.3093 - val_accuracy: 0.9149\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 3s 43us/step - loss: 0.3039 - accuracy: 0.9151 - val_loss: 0.2808 - val_accuracy: 0.9212\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2835 - accuracy: 0.9211 - val_loss: 0.2767 - val_accuracy: 0.9231\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2732 - accuracy: 0.9237 - val_loss: 0.2683 - val_accuracy: 0.9255\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2660 - accuracy: 0.9263 - val_loss: 0.2661 - val_accuracy: 0.9252\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2615 - accuracy: 0.9275 - val_loss: 0.2655 - val_accuracy: 0.9257\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2581 - accuracy: 0.9279 - val_loss: 0.2659 - val_accuracy: 0.9261\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2556 - accuracy: 0.9298 - val_loss: 0.2650 - val_accuracy: 0.9266\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2533 - accuracy: 0.9300 - val_loss: 0.2625 - val_accuracy: 0.9288\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2510 - accuracy: 0.9308 - val_loss: 0.2651 - val_accuracy: 0.9256\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.2491 - accuracy: 0.9315 - val_loss: 0.2649 - val_accuracy: 0.9281\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2474 - accuracy: 0.9314 - val_loss: 0.2669 - val_accuracy: 0.9278\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2461 - accuracy: 0.9320 - val_loss: 0.2701 - val_accuracy: 0.9258\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2453 - accuracy: 0.9320 - val_loss: 0.2673 - val_accuracy: 0.9274\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2438 - accuracy: 0.9326 - val_loss: 0.2660 - val_accuracy: 0.9288\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2428 - accuracy: 0.9337 - val_loss: 0.2687 - val_accuracy: 0.9273\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.2425 - accuracy: 0.9329 - val_loss: 0.2665 - val_accuracy: 0.9271\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.2411 - accuracy: 0.9334 - val_loss: 0.2682 - val_accuracy: 0.9284\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.2403 - accuracy: 0.9342 - val_loss: 0.2712 - val_accuracy: 0.9258\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2402 - accuracy: 0.9338 - val_loss: 0.2675 - val_accuracy: 0.9279\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2387 - accuracy: 0.9344 - val_loss: 0.2685 - val_accuracy: 0.9277\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.2383 - accuracy: 0.9340 - val_loss: 0.2724 - val_accuracy: 0.9268\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2377 - accuracy: 0.9342 - val_loss: 0.2705 - val_accuracy: 0.9270\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2374 - accuracy: 0.9347 - val_loss: 0.2711 - val_accuracy: 0.9274\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2367 - accuracy: 0.9347 - val_loss: 0.2696 - val_accuracy: 0.9275\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2364 - accuracy: 0.9345 - val_loss: 0.2704 - val_accuracy: 0.9279\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.2357 - accuracy: 0.9348 - val_loss: 0.2758 - val_accuracy: 0.9257\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.2353 - accuracy: 0.9360 - val_loss: 0.2737 - val_accuracy: 0.9276\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.2347 - accuracy: 0.9356 - val_loss: 0.2755 - val_accuracy: 0.9270\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 3s 50us/step - loss: 0.2341 - accuracy: 0.9358 - val_loss: 0.2762 - val_accuracy: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x201339bb688>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mlp_model = Sequential()\n",
    "my_mlp_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "\n",
    "###### YOUR LAYERS HERE\n",
    "\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dense(64, activation='tanh'))\n",
    "mlp_model.add(Dropout(0.20))\n",
    "\n",
    "# my_mlp_model.add(Dense(??, activation=??))\n",
    "# my_mlp_model.add(Dropout(??))\n",
    "\n",
    "############\n",
    "my_mlp_model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "my_mlp_model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "my_mlp_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the ConvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization, Activation\n",
    "\n",
    "conv_model = Sequential()\n",
    "conv_model.add(Conv2D(16, kernel_size=(3,3)))\n",
    "conv_model.add(BatchNormalization())\n",
    "conv_model.add(Activation('relu'))\n",
    "conv_model.add(Conv2D(16, kernel_size=(3,3),activation='relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "conv_model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "conv_model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the ConvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 70s 1ms/step - loss: 0.2116 - accuracy: 0.9359 - val_loss: 0.0880 - val_accuracy: 0.9730\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 0.0859 - accuracy: 0.9749 - val_loss: 0.0699 - val_accuracy: 0.9750\n",
      "Epoch 3/5\n",
      "32352/60000 [===============>..............] - ETA: 64:39:34 - loss: 0.0687 - accuracy: 0.9795"
     ]
    }
   ],
   "source": [
    "conv_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "conv_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 0.0639 - accuracy: 0.9809 - val_loss: 0.0483 - val_accuracy: 0.9856\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0448 - accuracy: 0.9863 - val_loss: 0.0518 - val_accuracy: 0.9825\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0351 - accuracy: 0.9885 - val_loss: 0.0438 - val_accuracy: 0.9855\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.0482 - val_accuracy: 0.9863\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0208 - accuracy: 0.9930 - val_loss: 0.0576 - val_accuracy: 0.9839\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 71s 1ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0622 - val_accuracy: 0.9850\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0571 - val_accuracy: 0.9853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17773469b88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "conv_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Build your own ConvNet\n",
    "Try different variations of layers, more convolutions, with/without dropout, more/less fully connected layer at the end. Report your best result. \n",
    "\n",
    "Optional: Can you build it without fully connected layers at the end? (Hint: Use number of channels in the output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "\n",
    "my_conv_model = Sequential()\n",
    "\n",
    "##### YOUR LAYERS HERE ######\n",
    "my_conv_model.add(Conv2D(??, kernel_size=(?,?)))\n",
    "my_conv_model.add(BatchNormalization())\n",
    "my_conv_model.add(MaxPooling2D(pool_size=(? ?)))\n",
    "\n",
    "#############################\n",
    "\n",
    "my_conv_model.compile(optimizer='sgd', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "my_conv_model.fit(x=x_train,y=y_train, validation_data=(x_test, y_test), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5ae5b26e1089>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_model\u001b[0m \u001b[1;31m# SET YOUR MODEL HERE (mlp_model, conv_model, my_mlp_model or my_conv_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrandom_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Greys'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'conv_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = conv_model # SET YOUR MODEL HERE (mlp_model, conv_model, my_mlp_model or my_conv_model)\n",
    "random_index = random.randint(0, x_test.shape[0])\n",
    "plt.imshow(x_test[random_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[random_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3 (optional): Try different normalization techniques and compare results with the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4 (optional): Try different optimizers (ADAM, RMSProp etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 5 (optional): Compare execution time for 10 epochs of an MLP and a ConvNet with the same number of layers. \n",
    "Which one is faster? How does this relate to the number of parameters? What about the computation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a list of conclusions here: \n",
    "\n",
    "E.g: Dropout is useless in ConvNets, BatchNorm is good etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
